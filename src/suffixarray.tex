\chapter{Suffix arrays}\label{ch:suffix-arrays}
Een tweede datastructuur die we in meer detail bekijken zijn suffix arrays, dit omwille van de lagere geheugenvereisten vergeleken met suffixbomen.

\section{Wat zijn suffix arrays?}\label{sec:wat-zijn-suffix-arrays?}
Suffix arrays zijn een geheugenefficiëntere voorstelling van suffixbomen.
In plaats van een boomstructuur maken we hier gebruik van een array die de volgnummers van elke suffix in de originele string bevat.
Deze volgnummers worden lexicografisch gesorteerd op basis van de overeenkomstige suffix.
Figuur~\ref{fig:suffixtree_vs_suffixarray} geeft een voorbeeld van een suffixboom en suffix array opgebouwd over de tekst \texttt{acacgt\$}.

\begin{center}
    \texttt{tekst: a|c|a|c|g|t|\$\\index: 0|1|2|3|4|5|6}
\end{center}
\begin{figure}[H]

    \begin{subfigure}[b]{0.6\linewidth}
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
            [
                level 1/.style = {sibling distance = 2.5cm},
                level 2/.style = {sibling distance = 1cm}
            ]

                \node[draw, circle] {}
                child {
                    [fill] circle (2pt)
                    edge from parent node [above] {6,7}
                }
                child {
                    node[draw, circle] {}
                    child {
                        [fill] circle (2pt)
                        edge from parent node [left] {2,7}
                    }
                    child {
                        [fill] circle (2pt)
                        edge from parent node [right] {4,7}
                    }
                    edge from parent node [below] {0,2}
                }
                child {
                    node[draw, circle] {}
                    child {
                        [fill] circle (2pt)
                        edge from parent node [left] {2,7}
                    }
                    child {
                        [fill] circle (2pt)
                        edge from parent node [right] {4,7}
                    }
                    edge from parent node [right] {1,2}
                }
                child {
                    [fill] circle (2pt)
                    edge from parent node [below] {4,7}
                }
                child {
                    [fill] circle (2pt)
                    edge from parent node [above] {5,7}
                }
                ;
            \end{tikzpicture}
        }
        \caption{Suffixboom}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
        \centering
        \begin{tikzpicture}[thick,scale=.6]
            \draw (0,0) grid (7,1);
            \path (.5,.5) node{$6$} foreach \i in {0,2,1,3,4,5} {++(1,0) node{$\i$}};
        \end{tikzpicture}
        \vspace{3em} % vertically center the array a bit
        \caption{Suffix array}
    \end{subfigure}

    \caption{Suffixboom en suffix array voor de string \texttt{acacgt\$}.}\label{fig:suffixtree_vs_suffixarray}
\end{figure}

Wanneer we de bladeren van de suffixboom van links naar rechts bekijken dan valt te zien dat dit overeen komt met de suffix array.
Dit is dan ook de link tussen deze twee datastructuren.
Onmiddellijk valt ook te zien dat een suffix array minder data bevat.
De interne knopen en suffix links uit de suffixboom ontbreken.
Indien deze informatie ook nodig is kan gebruik maakt worden van zogenaamde Enhanced Suffix Arrays (ESAs).
Hierbij worden naast de suffix array nog drie extra tabellen bijgehouden.
Deze worden de LCP, Child en suffix link table genoemd.


\section{Complexiteit}\label{sec:complexiteit}
Naïef kan het opbouwen van een suffix array aan de hand van traditionele sorteeralgoritmes zoals merge sort~\cite{mergeSort} in $O(n \log n)$ tijd en $O(n^2)$ geheugen.
Hierbij is $n$ de lengte van de tekst.
Ondertussen bestaan er echter verschillende algoritmes die een tijdscomplexiteit van $O(n)$ bereiken~\cite{sais, ko_alura, radixSA, dark_archon, libdivsufsort}.
Bovendien vereisen deze veel minder geheugen dan een equivalente suffixboom.
Sommige implementaties vereisen slechts $5n + O(1)$ geheugen~\cite{dark_archon, libdivsufsort}.


\section{Bestaande implementaties}\label{sec:bestaande-implementaties}
Aangezien er meerdere sterk geoptimaliseerde implementaties bestaan voor het opbouwen van een suffix array vergelijken we eerst de performantie van deze implementaties.
Op basis hiervan kan daarna beslist worden welke implementatie gebruikt wordt.
Tabel~\ref{tab:sa_building} bevat een overzicht van verschillende algoritmes waarbij voor sommige algoritmes verschillende implementaties getest zijn.

\begin{table}[H]
    \begin{minipage}{\linewidth}
        \centering
            \begin{tabular}{l l S[table-format=-2.2] S[table-format=-2.2] S[table-format=-1.2] S[table-format=-1.2]}
                Algoritme & Programmeertaal & \multicolumn{2}{c}{Tijd (in sec)} & \multicolumn{2}{c}{Geheugen (in GB)} \\
                \hline\hline
                &                      & {32 bit} & {64 bit} & {32 bit} & {64 bit} \\
                \cline{3-6}
                libdivsufsort\footnote{\url{https://github.com/y-256/libdivsufsort}}                                       & C                    & 15.01    & 15.97    & 1.03     & 1.86     \\
                libdivsufsort\footnote{\url{https://github.com/baku4/libdivsufsort-rs}}                                    & Rust + bindings to C & 16.00    & 15.52    & 1.03     & 1.86     \\
                libdivsufsort\footnote{\url{https://github.com/fasterthanlime/stringsearch/tree/master/crates/divsufsort}}  & Rust                 & 20.23    & {-}      & 1.03     & {-}      \\
                dark archon a4\footnote{\url{https://github.com/kvark/dark-archon}}                                        & C                    & 39.34    & {-}      & 1.09     & {-}      \\
                libsais\footnote{\url{https://github.com/IlyaGrebnov/libsais}}                                             & C                    & 6.38     & 6.46     & 1.03     & 1.86     \\
                SA-IS\footnote{\url{https://github.com/Tascate/Suffix-Arrays-in-CPP}}                                      & C++                  & 24.39    & {-}      & 3.80     & {-}      \\
                SA-IS\footnote{\url{https://github.com/sile/sais}}                                                         & C++                  & 18.73    & {-}      & 1.46     & {-}      \\
                radixSA\footnote{\url{https://github.com/mariusmni/radixSA64}}                                             & C++                  & 9.74     & 11.26    & 2.11     & 3.52     \\
                \hline
            \end{tabular}
        \caption{Uitvoeringstijden en maximaal geheugenverbruik voor het opbouwen van een suffix array aan de hand van verschillende algoritmes voor de Swiss-Prot eiwitdatabank.
        Indien er een 32 bit en 64 bit integer implementatie beschikbaar was werden deze allebei getest. Een - staat voor niet getest. Deze testen werden lokaal uitgevoerd op een M1 Pro MacBook Pro. De specificaties hiervan zijn terug te vinden in tabel~\ref{tab:macbook_hardware}.}
        \label{tab:sa_building}
    \end{minipage}
\end{table}

We kunnen concluderen dat libsais duidelijk de snelste implementatie is om Swiss-Prot te indexeren.
Samen met libdivsufsort gebruikt het de kleinste hoeveelheid geheugen.
Dit maakt libsais het beste algoritme om onze eiwitdatabanken te indexeren.
Bovendien zien we dat als we de 32 bit en 64 bit implementatie vergelijken van eenzelfde algoritme dat de uitvoeringstijden erg gelijkaardig zijn.
In het algemeen zijn de 64 bit versies een fractie trager.
Verder valt ook te zien dat het verschil tussen de C versie en Rust versie die bindings heeft naar de C code klein is.
De overhead van het oproepen van de C code uit Rust is dus minimaal.
Een ander voordeel dat libsais en libdivsufsort hebben (naast hun minimale geheugenverbruik) is dat ze allebei een 64 bit integer implementatie hebben.
Dit is belangrijk voor het indexeren van UniprotKB omdat de totale tekst langer is dan de maximale 32 bit integer.
Dit zorgt ervoor dat alle 32 bit integer implementaties onbruikbaar zijn voor dit einddoel.


\section{Toepassen van suffix arrays op een eiwitdatabank}\label{sec:toepassen-van-suffix-arrays-op-een-eiwitdatabank}
Het moeilijkste stuk van onze probleemstelling is het opbouwen van de suffix array.
Dit stuk kunnen we oplossen aan de hand van de algoritmes uit sectie~\ref{sec:bestaande-implementaties}.
Eens we die suffix array opgebouwd hebben blijft er echter nog een stuk van ons probleem over.
Om te beginnen moeten we nog een mapping maken van de gevonden suffixen naar het bijbehorende eiwit.
Op basis van dit eiwit wordt daarna de LCA gezocht.

\subsection{Bouwen van de suffix array}\label{subsec:bouwen-van-de-suffix-array}
Zoals in de inleiding van sectie~\ref{sec:probleemstelling} willen we gebruik maken van Rust vanwege de combinatie van \textit{memory safety} en hoge performantie.
We willen echter gebruik maken van de al bestaande geoptimaliseerde implementaties van algoritmes om een suffix array op te bouwen.
Om beide doelen te bereiken maken we gebruik van interoperabiliteit tussen Rust en C/C++.
Zo bestaan er al bindings\footnote{\url{https://crates.io/crates/libdivsufsort-rs}} van Rust naar de originele implementatie van libdivsufsort\cite{libdivsufsort} (in C).
Ook al blijkt uit het testen dat dit algoritme voor het opbouwen van de indexstructuur over een eiwitdatabank niet het snelste is, het geheugengebruik is wel minimaal.
Dit laat toe om te experimenteren met het opbouwen van een SA zonder al te veel extra werk, en al onmiddellijk te zien hoe het geheugengebruik evolueert.
\\ \\
Later is er voor gekozen om zelf nog een simple Rust wrapper te schrijven rond de libsais C code.
Dit gebruik makende van het \texttt{bindgen}\footnote{\url{https://crates.io/crates/bindgen}} crate.
Op deze manier is het ook mogelijk om gebruik te maken van de snellere libsais algoritme eens we wisten dat SAs een efficiënte en schaalbare oplossing waren voor de probleemstelling
\\ \\
Het nadeel van het gebruiken van deze bindings naar C code is dat het oproepen van de effectieve C code gebeurd in een \texttt{unsafe} blok.
Hierbij is het dus mogelijk dat er geheugenfouten in het programma sluipen.
Dit risico is echter miniem aangezien dit bestaande, geteste bibliotheken zijn.
Bovendien zijn we ook zeker dat eventuele geheugenfouten enkel hierdoor kunnen ontstaan.
Dit is dus een afweging tussen optimale performantie (waarbij we het wiel niet hoeven heruit te vinden), en garantie van \textit{memory safety}.

\subsection{Mapping van suffix naar proteïne}\label{subsec:mapping-van-suffix-naar-proteine}
Deze mapping kan op twee manieren gebeuren.
Een eerste optie is om expliciet voor elke suffix bij te houden bij welke proteïne die hoort.
Dit kan aan de hand van een array die even lang is als het aantal suffixen.
Het voordeel van deze aanpak is dat het vinden van de bijbehorende proteïne in $O(1)$ tijd kan, hiervoor is echter wel $O(m)$ geheugen nodig, met $m$ de lengte van de totale tekst.
\\ \\
De tweede optie is om enkel de eerste of laatste suffix per proteïne bij te houden.
Het voordeel van deze aanpak is dat er minder geheugen nodig is, meer precies $O(n)$ geheugen met $n$ het aantal proteïnen.
Het nadeel is dan weer dat het vinden van de bijbehorende proteïne trager is.
Zoeken neemt $O(\log n)$ tijd in beslag aan de hand van binair zoeken.
Hierbij is $n$ het aantal proteïnes.
\\ \\
In figuur~\ref{fig:dense_vs_sparse} wordt de uitvoeringstijd voor beide implementaties vergeleken.
Standaard (en in alle komende testen) zullen wij gebruik maken van de sparse versie aangezien de performantie impact in de praktijk erg beperkt is, en er een significante hoeveelheid geheugengebruik uitgespaard kan worden.
Het grote verschil bij het Human-Prot zoekbestand valt te verklaren vanwege het extreem hoog aantal matches dat daar te vinden is voor de korte peptides.
In de praktijk zullen we dit maximaal aantal matches echter beperken (zie sectie~\ref{subsec:maximaal-aantal-matches}).
Dit zal op zijn beurt ook de overhead van de sparse mapping beperkt houden.
\begin{figure}[H]
    \centering
    \subfloat[Tijd nodig om alle matches te zoeken.]{\includegraphics[width=\linewidth]{dense_vs_sparse_time}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Maximaal gebruikt geheugen tijdens het zoeken naar alle matches.]{\includegraphics[width=\linewidth]{dense_vs_sparse_memory}}
    \caption{Zoektijd en geheugengebruik bij het gebruik bij een dense of sparse mapping van de suffixen naar de proteïnes.}\label{fig:dense_vs_sparse}
\end{figure}

\subsection{Berekenen van de LCA}\label{subsec:berekenen-van-de-lca}
Zoals eerder vermeld bevat een suffix array geen informatie over de interne toppen die voorkomen bij een suffixboom.
Dit zorgt ervoor dat het niet mogelijk is om op basis hiervan de LCA van de organismen voor te berekenen voor al deze interne toppen.
In de plaats is dit nu iets dat \textit{on the fly} moet gebeuren tijdens het zoekproces zelf.
Aangezien dit nu niet meer op voorhand berekend wordt, is er geen reden om LCA te verkiezen boven LCA*.
LCA* was namelijk onze eerste keuze, maar bij suffixbomen zijn we daarvan afgestapt om het voorberekenen efficiënter te maken.

% TODO: we kunnen ook gebruik maken van enhanded suffix arrays en dat dan wel proberen doen, maar haalbaarheid daarvan is nog uit te zoeken.


\section{Sparse en compressed suffix arrays}\label{sec:sparse-en-compressed-suffix-arrays}
Om het geheugenverbruik van suffix arrays verder te verkleinen kan er gebruik gemaakt worden van sparse of compressed suffix arrays.
In principe doen ze allebei hetzelfde, er wordt namelijk slechts een stuk van de originele suffix array bijgehouden.
Het verschil zit in welk stuk bijgehouden wordt.
\\ \\
Sparse suffix arrays (SSAs) bouwen een suffix array op basis van elke k-de suffix van de input tekst.
Bij compressed suffix arrays (CSAs) wordt daarentegen slechts elke k-de waarde van de SA bijgehouden.
Voor beide opties is de populairste manier om ze op te bouwen aan de hand van sampling op de volledige SA\@.
Hierdoor blijft het maximale geheugenverbruik tijdens het opbouwen identiek aan het gebruik van de volledige SA\@.
Dit is net het punt waar wij ons geheugenverbruik verder willen verlagen.
Gelukkig heeft het gebruik hiervan nog altijd het voordeel dat de uiteindelijke machine die de index zal hosten lagere geheugenvereisten zal hebben.
\\ \\
Het samplen is de meest gebruikte methode tot op vandaag vanwege de bestaande sterk geoptimaliseerde implementaties van de klassieke SA constructie algoritmes.
Bij sparse suffix arrays is het beste algoritme qua tijdscomplexiteit tot nu toe een Monte Carlo algoritme dat $O(n)$ tijd en $O(b)$ geheugen nodig heeft en een Las Vegas algoritme dat $O(n \sqrt{\log b})$ tijd en $O(b)$ geheugen verbruikt.
Hierbij is $n$ de lengte van de tekst, en $b$ het aantal effectief gebruikte suffixes in de sparse SA~\cite{building_sparse_sa}.
Van het Monte-Carlo algoritme is er een bestaand implementatie\footnote{\url{https://github.com/lorrainea/SSA/tree/main/MA}}.
Wanneer we aan de hand hiervan een SSA met sparseness factor 3 voor Swiss-Prot opbouwen blijkt dit niet alleen trager ($\pm$ 10 minuten) te zijn.
Ook het geheugengebruik ligt merkelijk hoger ($\pm$ 10 GB).
Terwijl we slechts een tiental seconden nodig hebben om de standaard SA op te bouwen in combinatie met 1.8 GB RAM\@.
Voor compressed suffix arrays bestaat er een algoritme dat een tijdscomplexiteit van $o(n)$ heeft in combinatie met $O(n \log \sigma)$ bits aan geheugen~\cite{building_compressed_sa}.
Hierbij is $n$ de tekstlengte en $\sigma$ de alfabetgrootte.
\\ \\
Het grootste nadeel aan deze algoritmes in context van deze thesis is dat er nog geen (sterk geoptimaliseerde) implementaties bestaan.
Bovendien zal de factor van ingevoegde sparseness in ons geval altijd vrij klein zijn om de zoektijden beperkt te houden (aangezien we werken met een erg grote dataset en vrij korte strings).
Indien we dus een implementatie hebben om rechtstreeks een CSA of SSA te bouwen, maar met een grotere constante qua geheugengebruik zal vanwege de kleine sparseness factor de winst snel verloren gaan.
Bovendien laat een CSA niet toe om rechtstreeks gebruik makende van de CSA te zoeken, er zijn nog extra hulp structuren nodig, wat opnieuw extra geheugen vraagt. % TODO: vraag eens aan peter hoe dat werkt, of dubbel check nog eens online, maar is weinig van te vinden
Daarom zijn SSAs interessanter in ons geval aangezien wij matches willen zoeken.
Deze laten toe om enkel met behulp van de tekst en de SSA te zoeken.

\subsection{Zoeken in sparse suffix arrays}\label{subsec:zoeken-in-sparse-suffix-arrays}
Het zoeken in een SSA is erg gelijkaardig aan het zoeken in een volledige SA\@.
Een belangrijke restrictie is echter dat in de SSA geen strings gezocht kunnen worden die kleiner dan of gelijk zijn aan de sparseness factor.
Bovendien heeft deze sparseness factor ook een erg belangrijke impact op de zoekperformantie.
Bij het zoeken met sparseness factor $k$ moeten we namelijk alle suffixen zoeken die matchen met de gezochte string waarbij de eerste $p \in [0, k-1]$ tekens overgeslagen worden.
Voor elke matchende suffix (stel dat dit suffix $s$ is) moet daarna gecontroleerd worden als de overgeslagen prefix van $p$ matcht met de $p$ tekens die op posities $[s - p, p[$ in de geïndexeerde tekst staan.
Als dit zo is, dan matcht suffix $s - p$ met de gezochte string.
Om alles iets duidelijker te maken volgt een klein voorbeeld waarin we de peptide \texttt{acg} in de tekst \texttt{acacgt\$} zoeken.
De index die gebruikt wordt om te zoeken is een SSA met sparseness factor 3.
Deze valt terug te vinden in figuur~\ref{fig:sparse_sa}.

\begin{center}
    \texttt{tekst: a|c|a|c|g|t|\$\\index: 0|1|2|3|4|5|6}
\end{center}
\begin{figure}[H]
    \hfill
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \begin{tikzpicture}[thick,scale=.6]
            \draw (0,0) grid (7,1);
            \path (.5,.5) node{$6$} foreach \i in {0,2,1,3,4,5} {++(1,0) node{$\i$}};
        \end{tikzpicture}
        \caption{Suffix array}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \begin{tikzpicture}[thick,scale=.6]
            \draw (0,0) grid (3,1);
            \path (.5,.5) node{$6$} foreach \i in {0,3} {++(1,0) node{$\i$}};
        \end{tikzpicture}
        \caption{Sparse suffix array met sparseness factor 3.}
    \end{subfigure}
    \hfill
    \caption{SA en SSA voor de tekst \texttt{acacgt\$}.}
    \label{fig:sparse_sa}
\end{figure}

\begin{enumerate}
    \item Sla 0 tekens over.
    Zoek \texttt{acg} in de SSA\@.
    We vinden geen matches.
    \item Sla 1 teken over.
    Dit wil zeggen dat we \texttt{cg} zoeken in de SSA\@.
    Dit levert een match op met suffix 3 (op index 1 in de SSA).
    \begin{enumerate}
        \item Controleer als de suffix die deels gematcht is ook volledig matcht met onze zoekstring.
        Dit wil zeggen dat we de overgeslagen prefix van $p = 1$ tekens (in dit geval de letter \texttt{a}) moeten kunnen matchen met de eerste $p$ tekens van suffix $3 - 1 = 2$.
        \item We zien dat het eerste teken van suffix 2 inderdaad een \texttt{a} is.
        Hiervoor is de SA zelf niet nodig, dit kan rechtstreeks via tekst gecontroleerd worden aan de hand van $p$ karakters die vergeleken moeten worden.
        Suffix 2 is dus een match voor de zoekstring \texttt{acg}.
    \end{enumerate}
    \item Sla 2 tekens over.
    Zoek \texttt{g} in de SSA\@.
    We vinden geen matches.
    \item We concluderen dat enkel suffix 2 matcht met de gezochte string \textit{acg}.
\end{enumerate}

\subsubsection{Performantie en indexgrootte}
Afhankelijk van de sparseness factor vraagt het zoeken meer iteraties.
Bovendien zal een grotere sparseness factor impliceren dat we iteraties zullen hebben waarbij de overgeslagen prefix langer is.
Dit op zijn beurt zorgt ervoor dat er kortere stukken in de SSA gezocht zullen worden, die in het algemeen meer matches zullen opleveren.
Voor al deze matches moeten we controleren als de tekens die ervoor komen matchen met de overgeslagen prefix.
Figuur~\ref{fig:search_sparseness} (a) visualiseert de impact hiervan op de zoektijd van het Swiss-Prot zoekbestand met en zonder missed cleavages.
Deze bestanden worden hiervoor gebruikt omdat de kortste sequentie die ze bevatten 5 aminozuren lang is.
Hierdoor blijft voor het gebruikte interval nog steeds elke peptide zoekbaar.
Zoals eerder al aangehaald kunnen peptides die minder lang zijn dan de sparseness factor niet meer gezocht worden.
Indien we het zoekbestand voor Human-Prot zouden gebruiken zou de evolutie van de zoektijd minder representatief zijn.
Dit omdat er een deel van de peptides korter zijn dan 5 aminozuren lang, deze zouden dus overgeslagen moeten worden.
\\
\begin{figure}[h]
    \centering
    \subfloat[Zoektijd voor het Swiss-Prot zoekbestand met en zonder missed cleavages in een sparse suffix array gebouwd aan de hand van de Swiss-Prot eiwitdatabank.]{\includegraphics[width=\linewidth]{swissprot_searchtime_sparseness}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Grootte van de volledige index en SSA voor de Swiss-Prot databank gebruik makende van verschillende sparseness factors.]{\includegraphics[width=0.8\linewidth]{index_size_SSA}}
    \caption{Zoektijd en indexgrootte voor Swiss-Prot.}\label{fig:search_sparseness}
\end{figure}

De reden dat de zoektijd zo explodeert wanneer we de sparseness factor verhogen is omdat er een deel van de gezochte peptides bestaat uit 5 aminozuren.
Wanneer we deze proberen zoeken in een SSA een sparseness factor 5, zoeken we eigenlijk enkel één letter in de SSA (de laatste van de peptide).
Deze zal erg veel matches opleveren, en op zijn beurt erg veel werk vragen om alle prefixen hiervan te controleren.
\\ \\
Anderzijds zal het verhogen van de sparseness factor de indexgrootte verkleinen.
Hierbij is het belangrijk dat enkel de SA mee verkleint.
Er blijft altijd een vaste hoeveelheid geheugen nodig om de tekst zelf nog op te slaan.
Figuur~\ref{fig:search_sparseness} (b) toont dit.
Er is duidelijk te zien dat voor dit geval de overgang van sparseness factor 4 naar 5 erg weinig winst heeft in opslag, maar een grote impact heeft op de performantie.
\\ \\
Er zijn dus twee erg belangrijke bevindingen over sparse suffix arrays, en de manier waarop wij ze opbouwen
\begin{enumerate}
    \item Probeer de sparseness factor zo laag mogelijk te houden, zo blijft ook de zoektijd voor kortere peptiden beperkt.
    \item Het maximale geheugenverbruik blijft constant onafhankelijk van de sparseness factor.
    De hoeveelheid geheugen om ze nadien te gebruiken kunnen we wel verkleinen, hierdoor kan de index op lichtere machines gebruikt worden na het opbouwen.
\end{enumerate}

Dit impliceert dat we de factor enkel moeten verhogen om het geheugenverbruik te beperken bij een al opgebouwde indexstructuur.
Dit is vooral van toepassing voor UniProtKB waar het nuttig is om kort een krachtige machine te gebruiken die de SA bouwt, waarna een minder krachtige machine de SSA host.
In het perfecte geval wordt de sparseness factor zo gekozen zodat de SSA samen met de tekst net in het RAM geheugen past van deze minder krachtige machine.

\section{Parallellisatie}\label{sec:parallellisatie}
Om het zoeken nog verder te versnellen kan gebruik gemaakt worden van parallellisatie.
We hebben namelijk een groot aantal peptides waarvoor we elke keer dezelfde statische indexstructuur moeten doorzoeken.
Rust maakt dit proces vrij simpel omdat het \textit{ownership} systeem dataraces voorkomt (behalve wanneer gebruik gemaakt wordt van \textit{unsafe} code of het \textit{interior mutability} patroon)\cite{rust_data_races}.
Om een datatype te gebruiken in combinatie met multithreading moet deze de \texttt{Sync} en \texttt{Send} trait implementeren.
Deze traits worden door het typesysteem automatisch afgeleid.
Als namelijk alle componenten van een type aan de \texttt{Sync} en \texttt{Send} trait voldoen, dan voldoet je nieuwe type ook automatisch.
\\ \\
Uiteindelijk hebben we twee geparallelliseerde implementaties gemaakt.
In de eerste wordt alles volledig zelf beheerd.
Hierbij verdelen we zelf welke data naar welke thread gaat, worden de threads manueel opgestart en sluiten we ze ook zelf af.
In de tweede implementatie wordt gebruik gemaakt van de Rayon crate\cite{rayon}.
Deze laat toe om op een simpele manier een sequentiële lus over een variabele te parallelliseren.
In ons geval was het omzetten van een sequentiële implementatie (nadat alle types voldeden aan de \texttt{Sync} en \texttt{Send} trait) zo simpel als het vervangen van \texttt{.iter()} door \texttt{.par\_iter()}.
Ook in deze implementatie is het mogelijk om manueel een specifiek aantal threads te kiezen.
Standaard gebruikt Rayen het aantal beschikbare logische cores op de machine, maar het instellen van een ander aantal kan aan de hand van één lijntje code.

\begin{minted}{Rust}
// Sequentieel
let results = peptides
    .iter()
    .map(|peptide| search_peptide(peptide))
    .collect();

// Parallel
let results = peptides
    .par_iter()
    .map(|peptide| search_peptide(peptide))
    .collect();
\end{minted}

\subsection{Manueel threaden vs Rayon}\label{subsec:manueel-threaden-vs-rayon}
Aangezien we twee verschillende implementaties hebben is het interessant om na te gaan hoe deze allebei presteren.
Figuur~\ref{fig:threading_default_vs_rayon} toont de evolutie van de zoektijden voor een verschillend aantal threads.
We zien duidelijk dat de versie die gebruikt maakt van Rayon net iets sneller is en dat beide implementaties $\pm$ lineair schalen.
De schaling is niet perfect 1--1 ten opzichte van het aantal threads omdat het inlezen en uitschrijven van de output sequentieel blijft.
Het verschil in uitvoeringstijd valt mogelijks te verklaren aan de manier waarop de data verdeeld wordt over de threads in combinatie met een efficiëntere manier om de resultaten uit de threads te verwerken.
In de eigen implementatie kreeg elke thread simpelweg $\frac{1}{x}$ van alle peptides toegekend, met $x$ het aantal threads.
De resultaten moeten daarna aan de hand van enkele stappen uit de thread-scope gehaald worden zodat deze terug beschikbaar zijn voor de rest van het programma.
Rayon maakt gebruik van een ingewikkelder systeem aan de hand van \textit{work stealing}\cite{rayon_stealing} om de data over de threads te verdelen.
Verder is het ophalen van de resultaten ook veel simpeler.
Deze worden rechtstreeks ter beschikking gesteld alsof er geen parallellisme was.

\begin{figure}[h]
    \centering
    \subfloat[Absolute uitvoeringstijd voor een verschillend aantal cores.]{\includegraphics[width=0.7\linewidth]{threading_default_vs_rayon}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Relatieve versnelling ten opzichte van uitvoering op 1 thread.]{\includegraphics[width=0.7\linewidth]{threading_default_vs_rayon_relative}}
    \caption{Tijdsmeting om het Swiss-Prot peptidebestand zonder missed cleavages te zoeken in een index met sparseness factor 3 ter grootte van 5\% van UniProtKB.}\label{fig:threading_default_vs_rayon}
\end{figure}

Naast het verschil in performantie zijn er nog andere voordelen verbonden aan het gebruik van Rayon.
De code is namelijk veel simpeler en daarom ook beter te onderhouden.
Dit motiveert onze keuze om finaal gebruik te maken van Rayon.

\section{Performantie}\label{sec:performantie}
Nu we verschillende manier verkend hebben om suffix arrays op te bouwen is het interessant om onze implementatie te vergelijken met suffixbomen.
Op basis hiervan kunnen we daarna vaststellen welke verbeteringen we gemaakt hebben, als deze indexstructuur goed genoeg schaalt om toepasbaar te zijn op de volledige UniProtKB databank en in welke tijd de peptides gezocht kunnen worden.

\subsection{Opbouwen}
Figuur~\ref{fig:array_building} visualiseert de tijd nodig om de indexstructuur op te bouwen in combinatie met het geheugenverbruik.
Er is duidelijk een mooie tijdswinst verkregen, wat een bonus is voor het lokaal opbouwen van indexen.
Voor het opbouwen van de index op UniProt is dit echter minder van belang aangezien dit proces slechts een 4 tal keren per jaar moet gebeuren.
Zolang de nodige CPU tijd hiervoor niet langer is dan enkele dagen is dit acceptabel.
Een veel belangrijkere vaststelling is het maximale geheugenverbruik tijdens opbouwen.
Ook deze is drastisch gedaald.
Dit is exact de reden dat we deze indexstuctuur gekozen hebben.
Wanneer we het resultaat voor Swiss-Prot extrapoleren naar UniProtKB gebruikmakende van de assumptie dat UniProtKB $\pm$ 500x groter is dan is de verwachting dat er ongeveer 1.2 TB RAM nodig is.
Dit is nog steeds een grote hoeveelheid, maar dit wordt al praktisch mogelijk op een server gericht op het uitvoeren van geheugenintensieve taken.

\begin{figure}[h]
    \centering
    \subfloat[Tijd nodig om de index op te bouwen.]{\includegraphics[width=\linewidth]{building_array_libsais_time}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Maximaal gebruikt geheugen tijdens het opbouwen van de suffixboom.]{\includegraphics[width=\linewidth]{building_array_libsais_memory}}
    \caption{Vergelijking tussen de nodige tijd en hoeveelheid geheugen om een suffix array met LibSais op te bouwen of een Suffixboom in onze eigen Rust implementatie. De tijd en het geheugengebruik zijn gemeten met het unix \texttt{time} commando. Als invoerbestand gebruiken we hier de Swiss-Prot of Human-Prot eiwitdatabank.}\label{fig:array_building}
\end{figure}

\subsection{Zoeken}
Aangezien er geen voorberekening gebeurd van LCA's is het enkel nuttig om de zoektijd inclusief het berekenen van de LCA te bekijken.
De tijd tot een match levert ons nog niet alle nodige informatie op (zoals bij de suffixboom).

\subsection{Beperken van het maximaal aantal matches}\label{subsec:maximaal-aantal-matches}
Op het eerste zicht leek de performantie voor sommige bestanden dramatisch veel slechter.
Dit is zichtbaar voor het Human-Prot zoekbestand in~\ref{fig:cutoff_humanprot} (a).
Na wat onderzoek bleek dat het berekenen van de LCA* erg traag werd indien er een groot aantal matches was.
Daarom is er besloten om een cut-off in te stellen.
Indien een peptide meer dan dat aantal matches heeft wordt verondersteld dat de root de kleinste gemeenschappelijke voorouder is van alle matches.
Uit onderzoek~\cite{unipept_cutoff} blijkt dat dit in de praktijk in de overgrote meerderheid van de gevallen ook effectief het geval is.
Zo zijn er slechts $\pm$ 13000 peptides met meer dan 10000 eiwiten matchen.
Hiervan was voor 95\% van de peptides de LCA de root en slechts voor 200 peptides een resultaat op \textit{species} niveau.
\\
\begin{figure}[h]
    \centering
    \subfloat[Bereken de LCA* voor alle matches.]{\includegraphics[width=0.7\linewidth]{no_cutoff_humanprot_search}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Bereken de LCA* als er minder dan 10000 matches zijn.]{\includegraphics[width=0.7\linewidth]{cutoff_humanprot_search}}
    \caption{Berekenen van de LCA* (inclusief zoeken) voor alle peptiden in Human-Prot zonder en met cut-off.}\label{fig:cutoff_humanprot}
\end{figure}

In figuur~\ref{fig:cutoff_humanprot} duidelijk te zien dat de uitvoeringstijd drastisch daalt wanneer de cut-off op 10000 matches geplaatst wordt.
Als we dit specifiek bekijken voor de Human-Prot peptidebestanden en eiwitdatabank is de uitvoeringstijd maar liefst een goede 300x sneller.
Bovendien is ook hier het informatieverlies minimaal.
Van de 250000 peptides zijn er 12 peptides die een ander resultaat verkrijgen in de output.
Deze 12 entries zijn echter slechts twee verschillende peptides (die gewoon meerdere malen voorkomen in het peptidebestand).
Dit zijn de peptide \texttt{EKP} en de peptide \texttt{SKE}.
Indien we de LCA* effectief berekenen is het resultaat in beide gevallen 9606, terwijl we met een cut-off de root (1) terug geven.
\\ \\
Nu we het zoeken in de suffix array afgesteld hebben aan de hand van een cut-off is het interessant om te kijken hoe de zoekperformantie zich verhoudt ten opzichte van het zoeken in een suffixboom.
Bij een 1 op 1 vergelijking is het zoeken in de suffixboom duidelijk sneller.
Dit is ook exact wat we verwacht hadden.
In de praktijk is het zoeken in een suffix array 5 tot 10 maal trager voor onze testbestanden.
Een deel van deze extra zoektijd kan opgevangen worden door gebruik te maken van parallel zoeken.
Indien we hier gebruik van maken neemt het zoeken opnieuw de orde van enkele tientallen tot honderden milliseconden voor 100000 peptides te zoeken.
Dit is echter ook iets dat mogelijk is in een suffixboom, alleen zal de verkregen tijdswinst in absolute tijd daar nog kleiner zijn.

\section{UniProtKB}
Zoals eerder vermeld levert een ruwe extrapolatie op dat we geschat 1.2 TB aan geheugen nodig zou hebben om een index voor UniProtKB op te bouwen.
Hierbij werd er echter van uitgegaan dat UniProtKB 500x meer eiwitten dan Swiss-Prot bevat.
Dit is een overschatting van de realiteit waar UniProt op dit moment \textit{slechts} $\pm$ 440 keer groter is.
Dit zorgt ervoor dat het opbouwen mogelijks al haalbaar is op HPC van UGent waar nodes beschikbaar zijn die ongeveer 940 GB aan beschikbaar geheugen hebben.
Na dit te proberen bleek dit ook effectief al haalbaar.
Figuur~\ref{fig:build_uniprot} toont de nodige tijd en hoeveelheid geheugen om dit te realiseren.
Opvallend hierbij is dat het Libsais algoritme hier trager is dan libdivsufsort, terwijl dit voor alle kleinere datasets net omgekeerd was.
Afhankelijk van je dataset is het ene algoritme dus sneller dan het andere.
Het geheugengebruik van beide algoritmes blijft echter erg gelijkaardig, wat het belangrijkste is voor ons.

\begin{figure}[h]
    \centering
    \subfloat[Tijd nodig om een index voor UniProtKB te bouwen.]{\includegraphics[width=\linewidth]{build_uniprot_time}}\\[4ex] % [4ex] om wat extra vertical spacing in te voegen

    \subfloat[Maximale hoeveelheid geheugen gebruikt tijdens het opbuowen van een index voor UniProtKB.]{\includegraphics[width=\linewidth]{build_uniprot_memory}}
    \caption{Statistieken voor het opbouwen van een SA voor UniProtKB}\label{fig:build_uniprot}
\end{figure}

Een volgende stap na het opbouwen van de volledige SA was het beslissen van de sparseness factor.
Zoals eerder aangegeven in de conclusie van sectie~\ref{subsec:zoeken-in-sparse-suffix-arrays} willen we deze sparseness factor zo laag mogelijk kiezen, met als restrictie dat de SSA nog steeds in het geheugen moet kunnen gehouden worden.
De beschikbare machines die UniPept hosten hebben elk $pm$ 0.5 TB RAM ter beschikking.
Dit wil zeggen dat we de resulterende index hierin moeten krijgen, en ook nog genoeg overhead moeten laten zodat de machine zeker niet out-of-memory gaat tijdens het hosten van de index en het verwerken van meerdere requests.
Praktisch gezien komt dit er op neer dat we gebruik maken van sparseness factor 3. % TODO: of 2, moeten we eens testen
Dit resulteert in een SA van 231.81 GB.
In combinatie met de tekst (86.93 GB) resulteert dit in een totale indexgrootte van 318,74 GB, wat comfortabel in de 0.5 TB past.
Indien we sparseness factor 2 zouden gebruiken is dit niet meer het geval. % TODO: zou er wel in kunnen, ongeveer 440 GB in het totaal nodig, maar is dit nog "oké?"
Dan komt de totale indexgrootte op $\pm$ 450 GB uit, wat te dicht ligt bij de grens.
